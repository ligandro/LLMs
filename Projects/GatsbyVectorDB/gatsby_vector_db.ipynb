{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32589371",
   "metadata": {},
   "source": [
    "# üìö The Great Gatsby - Vector Database Project\n",
    "\n",
    "This notebook demonstrates how to convert F. Scott Fitzgerald's \"The Great Gatsby\" into a searchable vector database using ChromaDB and perform semantic search queries.\n",
    "\n",
    "- Loading and preprocessing text data\n",
    "- Creating embeddings with SentenceTransformers\n",
    "- Storing vectors in ChromaDB\n",
    "- Performing semantic search queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4833b4",
   "metadata": {},
   "source": [
    "## 1. Install and Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61dba698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eca4c74",
   "metadata": {},
   "source": [
    "## 2. Download The Great Gatsby Text\n",
    "\n",
    "We'll download the text from Project Gutenberg (public domain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8cb0ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Downloading The Great Gatsby...\n",
      "‚úÖ Downloaded successfully!\n",
      "üìä Total characters: 270822\n",
      "üìä First 500 characters:\n",
      "*** START OF THE PROJECT GUTENBERG EBOOK 64317 ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                           The Great Gatsby\n",
      "                                  by\n",
      "                          F. Scott Fitzgerald\n",
      "\n",
      "\n",
      "                           Table of Contents\n",
      "\n",
      "I\n",
      "II\n",
      "III\n",
      "IV\n",
      "V\n",
      "VI\n",
      "VII\n",
      "VIII\n",
      "IX\n",
      "\n",
      "\n",
      "                              Once again\n",
      "                                  to\n",
      "                                 Zelda\n",
      "\n",
      "\n",
      "  Then wear the gold hat, if that will move her;\n",
      "  If you can bounce high, bounce for her too,\n",
      "  Till she cry ‚ÄúLover, go\n"
     ]
    }
   ],
   "source": [
    "# Download The Great Gatsby from Project Gutenberg\n",
    "url = \"https://www.gutenberg.org/files/64317/64317-0.txt\"\n",
    "data_path = Path(\"data/great_gatsby.txt\")\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "data_path.parent.mkdir(exist_ok=True)\n",
    "\n",
    "# Download the file if it doesn't exist\n",
    "if not data_path.exists():\n",
    "    print(\"üì• Downloading The Great Gatsby...\")\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    with open(data_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(response.text)\n",
    "    print(\"‚úÖ Downloaded successfully!\")\n",
    "else:\n",
    "    print(\"‚úÖ File already exists!\")\n",
    "\n",
    "# Read the file\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(f\"üìä Total characters: {len(raw_text)}\")\n",
    "print(f\"üìä First 500 characters:\\n{raw_text[:500]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0b2fa1",
   "metadata": {},
   "source": [
    "## 3. Clean and Preprocess the Text\n",
    "\n",
    "Remove Project Gutenberg headers/footers and clean the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "556de7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Cleaned text length: 269986 characters\n",
      "üìä First 500 characters:\n",
      "*** START OF THE PROJECT GUTENBERG EBOOK 64317 ***\n",
      "\n",
      " The Great Gatsby\n",
      " by\n",
      " F. Scott Fitzgerald\n",
      "\n",
      " Table of Contents\n",
      "\n",
      "I\n",
      "II\n",
      "III\n",
      "IV\n",
      "V\n",
      "VI\n",
      "VII\n",
      "VIII\n",
      "IX\n",
      "\n",
      " Once again\n",
      " to\n",
      " Zelda\n",
      "\n",
      " Then wear the gold hat, if that will move her;\n",
      " If you can bounce high, bounce for her too,\n",
      " Till she cry ‚ÄúLover, gold-hatted, high-bouncing lover,\n",
      " I must have you!‚Äù\n",
      "\n",
      " Thomas Parke d‚ÄôInvilliers\n",
      "\n",
      " I\n",
      "\n",
      "In my younger and more vulnerable years my father gave me some advice\n",
      "that I‚Äôve been turning over in my mind ever since.\n",
      "\n",
      "‚ÄúWhenev\n"
     ]
    }
   ],
   "source": [
    "# Clean the text - remove Project Gutenberg header and footer\n",
    "def clean_text(text):\n",
    "    # Find the start of the actual book (after the Gutenberg header)\n",
    "    start_markers = [\"CHAPTER I\", \"Chapter I\", \"CHAPTER 1\"]\n",
    "    start_idx = 0\n",
    "    for marker in start_markers:\n",
    "        idx = text.find(marker)\n",
    "        if idx != -1:\n",
    "            start_idx = idx\n",
    "            break\n",
    "    \n",
    "    # Find the end (before Gutenberg footer)\n",
    "    end_markers = [\"End of Project Gutenberg\", \"*** END OF THE PROJECT GUTENBERG\"]\n",
    "    end_idx = len(text)\n",
    "    for marker in end_markers:\n",
    "        idx = text.find(marker)\n",
    "        if idx != -1:\n",
    "            end_idx = idx\n",
    "            break\n",
    "    \n",
    "    # Extract the main text\n",
    "    clean = text[start_idx:end_idx]\n",
    "    \n",
    "    # Clean up extra whitespace\n",
    "    clean = re.sub(r'\\n{3,}', '\\n\\n', clean)\n",
    "    clean = re.sub(r' {2,}', ' ', clean)\n",
    "    \n",
    "    return clean.strip()\n",
    "\n",
    "cleaned_text = clean_text(raw_text)\n",
    "print(f\"üìä Cleaned text length: {len(cleaned_text)} characters\")\n",
    "print(f\"üìä First 500 characters:\\n{cleaned_text[:500]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b61ce8",
   "metadata": {},
   "source": [
    "## 4. Split Text into Chunks\n",
    "\n",
    "We'll split the text into paragraphs for better semantic search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2e28a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Number of chunks: 769\n",
      "üìä Average chunk length: 289 characters\n",
      "\n",
      "üìù Sample chunk (first one):\n",
      "Then wear the gold hat, if that will move her;\n",
      " If you can bounce high, bounce for her too,\n",
      " Till she cry ‚ÄúLover, gold-hatted, high-bouncing lover,\n",
      " I must have you!‚Äù...\n",
      "\n",
      "üìù Sample chunk (middle one):\n",
      "‚ÄúThey‚Äôre such beautiful shirts,‚Äù she sobbed, her voice muffled in the\n",
      "thick folds. ‚ÄúIt makes me sad because I‚Äôve never seen such‚Äîsuch\n",
      "beautiful shirts before.‚Äù...\n"
     ]
    }
   ],
   "source": [
    "# Split into paragraphs (chunks)\n",
    "def split_into_chunks(text, min_length=100):\n",
    "    \"\"\"Split text into paragraphs, filtering out very short ones.\"\"\"\n",
    "    # Split by double newlines (paragraphs)\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    \n",
    "    # Filter out very short paragraphs\n",
    "    chunks = [p.strip() for p in paragraphs if len(p.strip()) >= min_length]\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "chunks = split_into_chunks(cleaned_text)\n",
    "print(f\"üìä Number of chunks: {len(chunks)}\")\n",
    "print(f\"üìä Average chunk length: {sum(len(c) for c in chunks) // len(chunks)} characters\")\n",
    "print(f\"\\nüìù Sample chunk (first one):\\n{chunks[0][:300]}...\")\n",
    "print(f\"\\nüìù Sample chunk (middle one):\\n{chunks[len(chunks)//2][:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ccaa01",
   "metadata": {},
   "source": [
    "## 5. Initialize ChromaDB Client\n",
    "\n",
    "Create a persistent client to store our vector database locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "165c0501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ChromaDB client initialized!\n"
     ]
    }
   ],
   "source": [
    "# Initialize ChromaDB with persistent storage\n",
    "chroma_client = chromadb.PersistentClient(path=\"./vector_db\")\n",
    "print(\"‚úÖ ChromaDB client initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f2ed9a",
   "metadata": {},
   "source": [
    "## 6. Create Sentence Transformer Embedding Function\n",
    "\n",
    "Initialize the embedding model using SentenceTransformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "575cbe0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\lolen\\anaconda3\\envs\\llm\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "‚úÖ Sentence Transformer embedding function created!\n",
      "üìä This model creates 384-dimensional embeddings\n"
     ]
    }
   ],
   "source": [
    "# Create embedding function using SentenceTransformer\n",
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Sentence Transformer embedding function created!\")\n",
    "print(\"üìä This model creates 384-dimensional embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e13a47",
   "metadata": {},
   "source": [
    "## 7. Create Vector Embeddings\n",
    "\n",
    "Generate embeddings for all text chunks. This may take a minute..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac70002e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Generating embeddings for 769 chunks...\n",
      "‚úÖ Generated 769 embeddings!\n",
      "üìä Each embedding has 384 dimensions\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for all chunks\n",
    "print(f\"üîÑ Generating embeddings for {len(chunks)} chunks...\")\n",
    "vectors = sentence_transformer_ef(chunks)\n",
    "\n",
    "print(f\"‚úÖ Generated {len(vectors)} embeddings!\")\n",
    "print(f\"üìä Each embedding has {len(vectors[0])} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f24e44",
   "metadata": {},
   "source": [
    "## 8. Store Embeddings in ChromaDB Collection\n",
    "\n",
    "Create a collection and add all chunks with their embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b02bf267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Adding 769 chunks to ChromaDB...\n",
      "‚úÖ Successfully added documents to collection!\n",
      "üìä Collection count: 769\n"
     ]
    }
   ],
   "source": [
    "# Create unique IDs for each chunk\n",
    "ids = [f\"chunk_{i}\" for i in range(len(chunks))]\n",
    "\n",
    "# Uncomment to delete existing collection if needed\n",
    "# chroma_client.delete_collection(name=\"great_gatsby\")\n",
    "\n",
    "# Create or get collection\n",
    "collection = chroma_client.get_or_create_collection(name=\"great_gatsby\")\n",
    "\n",
    "print(f\"üì¶ Adding {len(chunks)} chunks to ChromaDB...\")\n",
    "\n",
    "# Add documents to collection\n",
    "collection.add(\n",
    "    documents=chunks,\n",
    "    ids=ids,\n",
    "    embeddings=vectors,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Successfully added documents to collection!\")\n",
    "print(f\"üìä Collection count: {collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5ead21",
   "metadata": {},
   "source": [
    "## 9. Query the Vector Database\n",
    "\n",
    "Now let's search for relevant passages using semantic search!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "227e6f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Query: Gatsby's extravagant parties with music and dancing\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üìÑ Result 1 (Similarity: 0.3714)\n",
      "--------------------------------------------------------------------------------\n",
      "I believe that on the first night I went to Gatsby‚Äôs house I was one\n",
      "of the few guests who had actually been invited. People were not\n",
      "invited‚Äîthey went there. They got into automobiles which bore them out\n",
      "to Long Island, and somehow they ended up at Gatsby‚Äôs door. Once there\n",
      "they were introduced by ...\n",
      "\n",
      "\n",
      "üìÑ Result 2 (Similarity: 0.2974)\n",
      "--------------------------------------------------------------------------------\n",
      "Daisy and Gatsby danced. I remember being surprised by his graceful,\n",
      "conservative foxtrot‚ÄîI had never seen him dance before. Then they\n",
      "sauntered over to my house and sat on the steps for half an hour,\n",
      "while at her request I remained watchfully in the garden. ‚ÄúIn case\n",
      "there‚Äôs a fire or a flood,‚Äù she ...\n",
      "\n",
      "\n",
      "üìÑ Result 3 (Similarity: 0.1896)\n",
      "--------------------------------------------------------------------------------\n",
      "Her glance left me and sought the lighted top of the steps, where\n",
      "‚ÄúThree O‚ÄôClock in the Morning,‚Äù a neat, sad little waltz of that year,\n",
      "was drifting out the open door. After all, in the very casualness of\n",
      "Gatsby‚Äôs party there were romantic possibilities totally absent from\n",
      "her world. What was it up...\n",
      "\n",
      "\n",
      "üìÑ Result 4 (Similarity: 0.1469)\n",
      "--------------------------------------------------------------------------------\n",
      "‚ÄúHe becomes very sentimental sometimes,‚Äù explained Gatsby. ‚ÄúThis is\n",
      "one of his sentimental days. He‚Äôs quite a character around New York‚Äîa\n",
      "denizen of Broadway.‚Äù\n",
      "\n",
      "\n",
      "üìÑ Result 5 (Similarity: 0.1408)\n",
      "--------------------------------------------------------------------------------\n",
      "I had been actually invited. A chauffeur in a uniform of robin‚Äôs-egg\n",
      "blue crossed my lawn early that Saturday morning with a surprisingly\n",
      "formal note from his employer: the honour would be entirely Gatsby‚Äôs,\n",
      "it said, if I would attend his ‚Äúlittle party‚Äù that night. He had seen\n",
      "me several times, and ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query 1: Find passages about Gatsby's parties\n",
    "query = \"Gatsby's extravagant parties with music and dancing\"\n",
    "query_embedding = sentence_transformer_ef([query])\n",
    "\n",
    "results = collection.query(\n",
    "    query_embeddings=query_embedding,\n",
    "    n_results=5, # how many results to return\n",
    ")\n",
    "\n",
    "print(\"üîç Query:\", query)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "for i, (doc, distance) in enumerate(zip(results['documents'][0], results['distances'][0])):\n",
    "    print(f\"\\nüìÑ Result {i+1} (Similarity: {1-distance:.4f})\")\n",
    "    print(\"-\" * 80)\n",
    "    print(doc[:300] + \"...\" if len(doc) > 300 else doc)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c225b93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Query: the green light at the end of the dock\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üìÑ Result 1 (Similarity: 0.2204)\n",
      "--------------------------------------------------------------------------------\n",
      "‚ÄúIf it wasn‚Äôt for the mist we could see your home across the bay,‚Äù\n",
      "said Gatsby. ‚ÄúYou always have a green light that burns all night at\n",
      "the end of your dock.‚Äù\n",
      "\n",
      "\n",
      "üìÑ Result 2 (Similarity: 0.0462)\n",
      "--------------------------------------------------------------------------------\n",
      "With an effort Wilson left the shade and support of the doorway and,\n",
      "breathing hard, unscrewed the cap of the tank. In the sunlight his\n",
      "face was green.\n",
      "\n",
      "\n",
      "üìÑ Result 3 (Similarity: -0.2387)\n",
      "--------------------------------------------------------------------------------\n",
      "Daisy put her arm through his abruptly, but he seemed absorbed in what\n",
      "he had just said. Possibly it had occurred to him that the colossal\n",
      "significance of that light had now vanished forever. Compared to the\n",
      "great distance that had separated him from Daisy it had seemed very\n",
      "near to her, almost touching her. It had seemed as close as a star to\n",
      "the moon. Now it was again a green light on a dock. Hi...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query 2: Find passages about the green light\n",
    "query = \"the green light at the end of the dock\"\n",
    "query_embedding = sentence_transformer_ef([query])\n",
    "\n",
    "results = collection.query(\n",
    "    query_embeddings=query_embedding,\n",
    "    n_results=3,\n",
    ")\n",
    "\n",
    "print(\"üîç Query:\", query)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "for i, (doc, distance) in enumerate(zip(results['documents'][0], results['distances'][0])):\n",
    "    print(f\"\\nüìÑ Result {i+1} (Similarity: {1-distance:.4f})\")\n",
    "    print(\"-\" * 80)\n",
    "    print(doc[:400] + \"...\" if len(doc) > 400 else doc)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b84e08",
   "metadata": {},
   "source": [
    "## 11. Alternative Query Method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c350a0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': None,\n",
      " 'distances': [[0.6270289421081543, 0.6323080062866211, 0.6470474004745483]],\n",
      " 'documents': [['Something in her tone reminded me of the other girl‚Äôs ‚ÄúI '\n",
      "                'think he\\n'\n",
      "                'killed a man,‚Äù and had the effect of stimulating my '\n",
      "                'curiosity. I would\\n'\n",
      "                'have accepted without question the information that Gatsby '\n",
      "                'sprang from\\n'\n",
      "                'the swamps of Louisiana or from the lower East Side of New '\n",
      "                'York. That\\n'\n",
      "                'was comprehensible. But young men didn‚Äôt‚Äîat least in my '\n",
      "                'provincial\\n'\n",
      "                'inexperience I believed they didn‚Äôt‚Äîdrift coolly out of '\n",
      "                'nowhere and\\n'\n",
      "                'buy a palace on Long Island Sound.',\n",
      "                'There was a small picture of Gatsby, also in yachting '\n",
      "                'costume, on the\\n'\n",
      "                'bureau‚ÄîGatsby with his head thrown back defiantly‚Äîtaken '\n",
      "                'apparently\\n'\n",
      "                'when he was about eighteen.',\n",
      "                'It was a random shot, and yet the reporter‚Äôs instinct was '\n",
      "                'right.\\n'\n",
      "                'Gatsby‚Äôs notoriety, spread about by the hundreds who had '\n",
      "                'accepted his\\n'\n",
      "                'hospitality and so become authorities upon his past, had '\n",
      "                'increased all\\n'\n",
      "                'summer until he fell just short of being news. Contemporary '\n",
      "                'legends\\n'\n",
      "                'such as the ‚Äúunderground pipeline to Canada‚Äù attached '\n",
      "                'themselves to\\n'\n",
      "                'him, and there was one persistent story that he didn‚Äôt live '\n",
      "                'in a house\\n'\n",
      "                'at all, but in a boat that looked like a house and was moved '\n",
      "                'secretly\\n'\n",
      "                'up and down the Long Island shore. Just why these inventions '\n",
      "                'were a\\n'\n",
      "                'source of satisfaction to James Gatz of North Dakota, isn‚Äôt '\n",
      "                'easy to\\n'\n",
      "                'say.']],\n",
      " 'embeddings': None,\n",
      " 'ids': [['chunk_200', 'chunk_389', 'chunk_407']],\n",
      " 'included': ['metadatas', 'documents', 'distances'],\n",
      " 'metadatas': [[None, None, None]],\n",
      " 'uris': None}\n"
     ]
    }
   ],
   "source": [
    "# Alternative method - let ChromaDB handle the embedding\n",
    "results = collection.query(\n",
    "    query_texts=[\"Gatsby's mysterious wealth and background\"],\n",
    "    n_results=3,\n",
    ")\n",
    "\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896faa70",
   "metadata": {},
   "source": [
    "## 12. Database Statistics\n",
    "\n",
    "Let's check the statistics of our vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd2a9068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Vector Database Statistics\n",
      "================================================================================\n",
      "Collection Name: great_gatsby\n",
      "Total Documents: 769\n",
      "Embedding Dimensions: 384\n",
      "Embedding Model: all-MiniLM-L6-v2\n",
      "Storage Path: ./gatsby_vector_db\n",
      "\n",
      "‚úÖ The Great Gatsby is now fully searchable as a vector database!\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Vector Database Statistics\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Collection Name: {collection.name}\")\n",
    "print(f\"Total Documents: {collection.count()}\")\n",
    "print(f\"Embedding Dimensions: 384\")\n",
    "print(f\"Embedding Model: all-MiniLM-L6-v2\")\n",
    "print(f\"Storage Path: ./gatsby_vector_db\")\n",
    "print(\"\\n‚úÖ The Great Gatsby is now fully searchable as a vector database!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3214af7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
