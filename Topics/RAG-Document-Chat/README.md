# ğŸ“š RAG Document Chat 

A Streamlit-based AI-powered document assistant for PDF querying using LangChain, Ollama's Llama3.2, and ChromaDB.

<p align="center">
  <img width="70%" src="demo.png"> &nbsp &nbsp
</p>

---

## âš¡ Quick Demo

Upload any PDF â†’ Ask questions â†’ Get accurate, grounded answers

**What it does:**
- Automatically processes and chunks your PDF documents
- Embeds content into a searchable vector database
- Retrieves diverse, relevant information using MMR algorithm
- Generates answers grounded solely in document context
- Prevents hallucinations with anti-hallucination prompting

---

## ğŸ¯ Key Features

âœ… **No Hallucinations** - Answers only from document content  
âœ… **Intelligent Retrieval** - MMR algorithm for diverse, relevant results  
âœ… **Fast Processing** - Efficient PDF chunking and embedding  
âœ… **Multi-Query Support** - Alternative retrieval method available  
âœ… **Session Management** - Auto-cleanup between different PDFs  
âœ… **Interactive UI** - Streamlit interface with sidebar controls  
âœ… **Local LLM** - Runs entirely on Ollama (privacy-first)  
âœ… **Configurable** - Easy-to-modify settings in config.py  

---

## ğŸ—ï¸ How It Works

### Three Simple Stages

**Stage 1: Document Processing**
```
PDF Upload
    â†“
Extract Text
    â†“
Smart Chunking (1200 chars, 300 overlap)
    â†“
Vector Embeddings
```

**Stage 2: Intelligent Retrieval**
```
User Question
    â†“
Find Similar Context (MMR)
    â†“
Return Top 12 Diverse Chunks
```

**Stage 3: Grounded Answer**
```
Context + Question
    â†“
LLaMA 3.2 (temperature=0)
    â†“
Grounded Answer (no hallucinations)
```

### Why MMR Instead of Similarity?

**Basic Approach:** Pure semantic similarity  
â†’ Returns redundant chunks from same document section

**MMR Approach:** Maximal Marginal Relevance  
â†’ Balances relevance + diversity for comprehensive context (fetches 60, selects top 12)

---

## ğŸ“¦ Installation

### Requirements
- Python 3.10+
- Ollama installed and running
- 8GB RAM minimum

### Setup

**1. Install Ollama**
```bash
# Download from https://ollama.ai
# Pull required models:
ollama pull llama3.2
ollama pull nomic-embed-text
```

**2. Install dependencies**
```bash
pip install -r requirements.txt
```

---

## ğŸš€ Getting Started

**Start the application**
```bash
streamlit run app.py
```

App opens at: `http://localhost:8501`

**Usage:**
1. Upload a PDF file
2. Wait for processing (chunking + embedding)
3. Ask questions in the chat interface
4. Switch retrieval methods in sidebar (MMR/Multi-Query)

**That's it!** The system automatically cleans up when you upload a new PDF.

---

## ğŸ“š Project Structure

```
RAG-Document-Chat/
â”œâ”€â”€ ingest/                    # Document processing pipeline
â”‚   â”œâ”€â”€ load_pdf.py           # PDF loading & cleanup
â”‚   â”œâ”€â”€ chunk_documents.py    # Smart chunking
â”‚   â””â”€â”€ embed_chunks.py       # Embedding & ChromaDB
â”œâ”€â”€ rag/                       # RAG components
â”‚   â”œâ”€â”€ retriever.py          # MMR & Multi-Query retrievers
â”‚   â””â”€â”€ chain.py              # LLM chain with anti-hallucination
â”œâ”€â”€ config.py                  # Centralized configuration
â”œâ”€â”€ app.py                     # Streamlit UI
â””â”€â”€ README.md                  # This file
```

---

## ğŸ™ Acknowledgments

- **LLM:** [Ollama](https://ollama.ai/) - Local LLM runtime
- **Embeddings:** [Nomic Embed Text](https://ollama.ai/library/nomic-embed-text)
- **Vector DB:** [ChromaDB](https://www.trychroma.com/)
- **Framework:** [LangChain](https://www.langchain.com/)
- **UI:** [Streamlit](https://streamlit.io/)
- **Inspiration:** [EpsteinFiles-RAG](https://github.com/AnkitNayak-eth/EpsteinFiles-RAG) - MMR retrieval & anti-hallucination patterns

---
