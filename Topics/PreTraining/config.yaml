# Model Configuration
model:
  vocab_size: 5000        # Vocabulary size
  d_model: 256            # Embedding dimension
  n_heads: 8              # Number of attention heads
  n_layers: 4             # Number of transformer blocks
  d_ff: 1024              # Feed-forward dimension
  max_seq_len: 128        # Maximum sequence length
  dropout: 0.1            # Dropout rate

# Training Configuration
training:
  batch_size: 32
  learning_rate: 0.0001
  num_epochs: 10
  warmup_steps: 1000
  gradient_accumulation_steps: 1
  max_grad_norm: 1.0
  save_every: 2           # Save checkpoint every N epochs

# Data Configuration
data:
  train_split: 0.9
  val_split: 0.1
  min_word_freq: 2        # Minimum word frequency for vocabulary
